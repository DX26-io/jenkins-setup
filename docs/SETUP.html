<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<!--[if IE]><meta http-equiv="X-UA-Compatible" content="IE=edge"><![endif]-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 1.5.8">
<title>Sample setups</title>
<link rel="stylesheet" href="css/spring.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body class="article toc2 toc-left">
<div id="header">
<h1>Sample setups</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#kubernetes-setup">Kubernetes Setup</a>
<ul class="sectlevel2">
<li><a href="#kubernetes-cli-installation">Kubernetes CLI Installation</a></li>
<li><a href="#start-minikube-k8s">Kubernetes Cluster Setup</a></li>
<li><a href="#run-minikube">Run Minikube</a></li>
<li><a href="#certificates-and-workers">Certificates and Workers</a></li>
<li><a href="#generate-minikube-namespaces">Generate Minikube Namespaces</a></li>
</ul>
</li>
<li><a href="#the-demo-setup-cloud-foundry">The demo setup (Cloud Foundry)</a>
<ul class="sectlevel2">
<li><a href="#deploying-production-applications-to-pcf-dev">Deploying Production Applications to PCF Dev</a></li>
<li><a href="#running-prometheus-on-cf">Running Prometheus on CF</a></li>
<li><a href="#running-grafana-on-cf">Running Grafana on CF</a></li>
</ul>
</li>
<li><a href="#the-demo-setup-kubernetes">The demo setup (Kubernetes)</a>
<ul class="sectlevel2">
<li><a href="#deploying-production-applications-to-minikube">Deploying Production Applications to Minikube</a></li>
<li><a href="#running-prometheus-on-kubernetes">Running Prometheus on Kubernetes</a></li>
<li><a href="#running-grafana-on-kubernetes">Running Grafana on Kubernetes</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In these sections you will see examples of setups
for different platforms.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="kubernetes-setup"><a class="link" href="#kubernetes-setup">Kubernetes Setup</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>This section describes how to set up Kubernetes.</p>
</div>
<div class="sect2">
<h3 id="kubernetes-cli-installation"><a class="link" href="#kubernetes-cli-installation">Kubernetes CLI Installation</a></h3>
<div class="paragraph">
<p>First, you need to install the <code>kubectl</code> command-line interface (CLI).</p>
</div>
<div class="sect3">
<h4 id="kubernetes-cli-script"><a class="link" href="#kubernetes-cli-script">Script Installation</a></h4>
<div class="paragraph">
<p>You can use the <code>tools/k8s-helper.sh</code> script to install <code>kubectl</code>. To do so, run the following script:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ ./tools/minikube-helper download-kubectl</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Then the <code>kubectl</code> gets downloaded.</p>
</div>
</div>
<div class="sect3">
<h4 id="kubernetes-cli-manual"><a class="link" href="#kubernetes-cli-manual">Manual Installation</a></h4>
<div class="paragraph">
<p>You can perform a manual installation for either OSX or Linux.</p>
</div>
<div class="sect4">
<h5 id="example-for-osx"><a class="link" href="#example-for-osx">Example for OSX</a></h5>
<div class="paragraph">
<p>The following listing shows how to manually install on OSX:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl
----</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect4">
<h5 id="example-for-linux"><a class="link" href="#example-for-linux">Example for Linux</a></h5>
<div class="paragraph">
<p>The following listing shows how to manually install on Linux:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
$ chmod +x ./kubectl
$ sudo mv ./kubectl /usr/local/bin/kubectl</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>See <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">this page</a> for more information.</p>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="start-minikube-k8s"><a class="link" href="#start-minikube-k8s">Kubernetes Cluster Setup</a></h3>
<div class="paragraph">
<p>We need a cluster of Kubernetes. The best choice is <a href="https://github.com/kubernetes/minikube">Minikube</a>.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
You can skip this step if you have a Kubernetes cluster installed and do not
want to use Minikube. In that case, the only thing you have to do is to set up spaces.
</td>
</tr>
</table>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
Servers often run run out of resources at the stage step.
If that happens, <a href="#jenkins-resources-k8s">clear some apps from PCF Dev and continue</a>.
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="kubernetes-minikube-script"><a class="link" href="#kubernetes-minikube-script">Script Installation</a></h4>
<div class="paragraph">
<p>You can use the <code>tools/k8s-helper.sh</code> script to install <code>Minikube</code>. To do so, run the following script:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ ./tools/minikube-helper download-minikube</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Then the <code>Minikube</code> cluster gets downloaded.</p>
</div>
</div>
<div class="sect3">
<h4 id="kubernetes-minikube-manual"><a class="link" href="#kubernetes-minikube-manual">Manual Installation</a></h4>
<div class="paragraph">
<p>You can perform a manual installation for either OSX or Linux.</p>
</div>
<div class="sect4">
<h5 id="example-for-osx-2"><a class="link" href="#example-for-osx-2">Example for OSX</a></h5>
<div class="paragraph">
<p>The following listing shows how to manually install on OSX:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.20.0/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Feel free to skip running <code>sudo mv minikube /usr/local/bin</code> if you want to add minikube to your path manually.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="example-for-linux-2"><a class="link" href="#example-for-linux-2">Example for Linux</a></h4>
<div class="paragraph">
<p>The following listing shows how to manually install on Linux:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.20.0/minikube-linux-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>You can skip running <code>sudo mv minikube /usr/local/bin</code> if you want to add minikube to your path manually.
See <a href="https://github.com/kubernetes/minikube/releases">this page</a> for more information on the installation.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="run-minikube"><a class="link" href="#run-minikube">Run Minikube</a></h3>
<div class="paragraph">
<p>To start Kubernetes on your local box, run <code>minikube start</code>.</p>
</div>
<div class="paragraph">
<p>To add the dashboard, run <code>minikube dashboard</code>.</p>
</div>
</div>
<div class="sect2">
<h3 id="certificates-and-workers"><a class="link" href="#certificates-and-workers">Certificates and Workers</a></h3>
<div class="sect3">
<h4 id="minikube-certificates-and-workers"><a class="link" href="#minikube-certificates-and-workers">Minikube Certificates and Workers</a></h4>
<div class="paragraph">
<p>By default, if you install Minikube, all the certificates get installed in your
<code>~/.minikube</code> folder. Your <code>kubectl</code> configuration under <code>~/.kube/config</code> also
gets updated to use Minikube.</p>
</div>
</div>
<div class="sect3">
<h4 id="manual-certificates-and-workers-setup"><a class="link" href="#manual-certificates-and-workers-setup">Manual Certificates and Workers Setup</a></h4>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you want to run the default demo setup, you can skip this section.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To target a given Kubernetes instance, you need to pass around Certificate Authority
key and also user keys.</p>
</div>
<div class="paragraph">
<p>You can read more about the instructions on how to generate those keys <a href="https://coreos.com/kubernetes/docs/latest/openssl.html">here</a>.
Generally speaking, if you have a Kubernetes installation (such as <code>minikube</code>), this step
has already been done for you. Now you can reuse those keys on the workers.</p>
</div>
<div class="paragraph">
<p>The following inormation has been extracted from the <a href="https://coreos.com/kubernetes/docs/latest/configure-kubectl.html">Kubernetes official documentation</a>.</p>
</div>
<div class="paragraph">
<p>Configure <code>kubectl</code> to connect to the target cluster using the following commands, replacing the following values:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>${MASTER_HOST}</code> with the master node address or name used in previous steps.</p>
</li>
<li>
<p>Replace <code>${CA_CERT}</code> with the absolute path to the <code>ca.pem</code> created in previous steps.</p>
</li>
<li>
<p>Replace <code>${ADMIN_KEY}</code> with the absolute path to the <code>admin-key.pem</code> created in previous steps.</p>
</li>
<li>
<p>Replace <code>${ADMIN_CERT}</code> with the absolute path to the <code>admin.pem</code> created in previous steps.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following commands show how to perform these steps:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ kubectl config set-cluster default-cluster --server=https://${MASTER_HOST} --certificate-authority=${CA_CERT}
$ kubectl config set-credentials default-admin --certificate-authority=${CA_CERT} --client-key=${ADMIN_KEY} --client-certificate=${ADMIN_CERT}
$ kubectl config set-context default-system --cluster=default-cluster --user=default-admin
$ kubectl config use-context default-system</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="generate-minikube-namespaces"><a class="link" href="#generate-minikube-namespaces">Generate Minikube Namespaces</a></h3>
<div class="paragraph">
<p>With the Minikube cluster running, we need to generate namespaces. To do so, run the
<code>./tools/k8s-helper.sh setup-namespaces</code>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="the-demo-setup-cloud-foundry"><a class="link" href="#the-demo-setup-cloud-foundry">The demo setup (Cloud Foundry)</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The demo uses two applications: <a href="https://github.com/spring-cloud-samples/github-webhook/">Github Webhook</a>
and <a href="https://github.com/spring-cloud-samples/github-analytics/">Github analytics code</a>. The following
image shows how these application communicate with each other:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/CloudPipelines/jenkins/master/docs/images/demo/demo.png" alt="demo">
</div>
<div class="title">The overview of the demo: Github Webhook listens to HTTP calls and sends a message to Github Analytics</div>
</div>
<div class="paragraph">
<p>&#160;
&#160;</p>
</div>
<div class="paragraph">
<p>For the demo scenario we have two applications. <code>Github Analytics</code> and <code>Github Webhook</code>.
Let&#8217;s imagine a case where Github is emitting events via HTTP. <code>Github Webhook</code> has
an API that could register to such hooks and receive those messages. Once this happens
 <code>Github Webhook</code> sends a message by RabbitMQ to a channel. <code>Github Analytics</code> is
 listening to those messages and stores them in a MySQL database.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/CloudPipelines/jenkins/master/docs/images/demo/demo_metrics.png" alt="demo metrics">
</div>
<div class="title">Gathering metrics: Github Analytics exposes metrics that are polled by Prometheus</div>
</div>
<div class="paragraph">
<p>&#160;
&#160;</p>
</div>
<div class="paragraph">
<p><code>Github Analytics</code> has its KPIs (Key Performance Indicators) monitored. In the case
of that application the KPI is number of issues.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/CloudPipelines/jenkins/master/docs/images/demo/demo_alerting.png" alt="demo alerting">
</div>
<div class="title">Alerting over metrics: Grafana alerts Slack over Prometheus metrics</div>
</div>
<div class="paragraph">
<p>&#160;
&#160;</p>
</div>
<div class="paragraph">
<p>Let&#8217;s assume that if we go below the threshold of X issues then an alert should be
sent to Slack.</p>
</div>
<div class="sect2">
<h3 id="deploying-production-applications-to-pcf-dev"><a class="link" href="#deploying-production-applications-to-pcf-dev">Deploying Production Applications to PCF Dev</a></h3>
<div class="paragraph">
<p>In a real-world scenario, we would not want to automatically provision services such as
RabbitMQ, MySQL, or Eureka each time we deploy a new application to production. Typically,
production is provisioned manually (often by using automated solutions). In our case, before
you deploy to production, you can provision the <code>pcfdev-prod</code> space by using the
 <code>cf-helper.sh</code>. To do so, call the following script:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ ./cf-helper.sh setup-prod-infra</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>The CF CLI:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Logs in to PCF Dev,</p>
</li>
<li>
<p>Targets the <code>pcfdev-prod</code> space</p>
</li>
<li>
<p>Sets up:</p>
<div class="ulist">
<ul>
<li>
<p>RabbitMQ (under the <code>rabbitmq-github</code> name)</p>
</li>
<li>
<p>MySQL (under <code>mysql-github-analytics</code> name)</p>
</li>
<li>
<p>Eureka (under <code>github-eureka</code> name)</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="running-prometheus-on-cf"><a class="link" href="#running-prometheus-on-cf">Running Prometheus on CF</a></h3>
<div class="paragraph">
<p>You can check out <a href="https://github.com/making/prometheus-on-PCF">Toshiaki Maki&#8217;s code</a> on how to automate Prometheus installation on CF.</p>
</div>
<div class="paragraph">
<p>Go to <a href="https://prometheus.io/download/" class="bare">https://prometheus.io/download/</a> and download the Linux binary. Then run the following command:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cf push cloudpipelines-prometheus -b binary_buildpack -c './prometheus -web.listen-address=:8080' -m 64m</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Also, <code>localhost:9090</code> in <code>prometheus.yml</code> should be <code>localhost:8080</code>.</p>
</div>
<div class="paragraph">
<p>The file should resemble the following listing to work with the demo setup (change <code>github-analytics-cloud-pipelines.cfapps.io</code>
to your <code>github-analytics</code> installation).</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml"># my global config
global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: 'codelab-monitor'

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first.rules"
  # - "second.rules"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.
  - job_name: 'prometheus'

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ['localhost:8080']

  - job_name: 'demo-app'

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    metrics_path: '/prometheus'
    # scheme defaults to 'http'.

    static_configs:
      - targets: ['github-analytics-cloud-pipelines.cfapps.io']</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>A deployed version for the Cloud Pipelines demo is available <a href="https://cloudpipelines-prometheus.cfapps.io/">here</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="running-grafana-on-cf"><a class="link" href="#running-grafana-on-cf">Running Grafana on CF</a></h3>
<div class="paragraph">
<p>You can check out <a href="https://github.com/making/cf-grafana">Toshiaki Maki&#8217;s code</a> to see how to automate Prometheus installation on CF.</p>
</div>
<div class="paragraph">
<p>Download the tarball from <a href="https://grafana.com/grafana/download?platform=linux" class="bare">https://grafana.com/grafana/download?platform=linux</a>
and set <code>http_port = 8080</code> in <code>conf/default.ini</code>. Then run the following the command:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cf push cloudpipelines-grafana -b binary_buildpack -c './bin/grafana-server web' -m 64m</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>The demo uses Grafana Dashboard with an ID of <code>2471</code>.</p>
</div>
<div class="paragraph">
<p>A deployed version for the Cloud Pipelines demo is available <a href="https://cloudpipelines-grafana.cfapps.io/">here</a></p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="the-demo-setup-kubernetes"><a class="link" href="#the-demo-setup-kubernetes">The demo setup (Kubernetes)</a></h2>
<div class="sectionbody">
<div class="paragraph">
<p>The demo uses two applications: <a href="https://github.com/spring-cloud-samples/github-webhook-kubernetes/">Github Webhook</a>
and <a href="https://github.com/spring-cloud-samples/github-analytics-kubernetes/">Github analytics code</a>. The following
image shows how these application communicate with each other:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/CloudPipelines/jenkins/master/docs/images/demo/demo.png" alt="demo">
</div>
<div class="title">The overview of the demo: Github Webhook listens to HTTP calls and sends a message to Github Analytics</div>
</div>
<div class="paragraph">
<p>&#160;
&#160;</p>
</div>
<div class="paragraph">
<p>For the demo scenario we have two applications. <code>Github Analytics</code> and <code>Github Webhook</code>.
Let&#8217;s imagine a case where Github is emitting events via HTTP. <code>Github Webhook</code> has
an API that could register to such hooks and receive those messages. Once this happens
 <code>Github Webhook</code> sends a message by RabbitMQ to a channel. <code>Github Analytics</code> is
 listening to those messages and stores them in a MySQL database.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/CloudPipelines/jenkins/master/docs/images/demo/demo_metrics.png" alt="demo metrics">
</div>
<div class="title">Gathering metrics: Github Analytics exposes metrics that are polled by Prometheus</div>
</div>
<div class="paragraph">
<p>&#160;
&#160;</p>
</div>
<div class="paragraph">
<p><code>Github Analytics</code> has its KPIs (Key Performance Indicators) monitored. In the case
of that application the KPI is number of issues.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://raw.githubusercontent.com/CloudPipelines/jenkins/master/docs/images/demo/demo_alerting.png" alt="demo alerting">
</div>
<div class="title">Alerting over metrics: Grafana alerts Slack over Prometheus metrics</div>
</div>
<div class="paragraph">
<p>&#160;
&#160;</p>
</div>
<div class="paragraph">
<p>Let&#8217;s assume that if we go below the threshold of X issues then an alert should be
sent to Slack.</p>
</div>
<div class="sect2">
<h3 id="deploying-production-applications-to-minikube"><a class="link" href="#deploying-production-applications-to-minikube">Deploying Production Applications to Minikube</a></h3>
<div class="paragraph">
<p>In a real-world scenario, we would not want to automatically provision services such as
RabbitMQ, MySQL, or Eureka each time we deploy a new application to production. Typically,
production is provisioned manually (often by using automated solutions). In our case, before
you deploy to production, you can provision the <code>cloudpipelines-prod</code> namespace by using the
 <code>k8s-helper.sh</code>. To do so, call the following script:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ ./k8s-helper.sh setup-prod-infra</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="running-prometheus-on-kubernetes"><a class="link" href="#running-prometheus-on-kubernetes">Running Prometheus on Kubernetes</a></h3>
<div class="paragraph">
<p>Use Helm to install Prometheus. Later in this demo, we point it to the services
deployed to our cluster.</p>
</div>
<div class="paragraph">
<p>Create a file called <code>values.yaml</code> with the following content:</p>
</div>
<div class="exampleblock">
<div class="title">Example 1. values.yaml</div>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yml hljs" data-lang="yml">rbac:
  create: false

alertmanager:
  ## If false, alertmanager will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## alertmanager container name
  ##
  name: alertmanager

  ## alertmanager container image
  ##
  image:
    repository: prom/alertmanager
    tag: v0.9.1
    pullPolicy: IfNotPresent

  ## Additional alertmanager container arguments
  ##
  extraArgs: {}

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  baseURL: ""

  ## Additional alertmanager container environment variable
  ## For instance to add a http_proxy
  ##
  extraEnv: {}

  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}}
  ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: ""

  ingress:
    ## If true, alertmanager Ingress will be created
    ##
    enabled: false

    ## alertmanager Ingress annotations
    ##
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## alertmanager Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - alertmanager.domain.com

    ## alertmanager Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - alertmanager.domain.com

  ## Alertmanager Deployment Strategy type
  # strategy:
  #   type: Recreate

  ## Node labels for alertmanager pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  persistentVolume:
    ## If true, alertmanager will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## alertmanager data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## alertmanager data Persistent Volume Claim annotations
    ##
    annotations: {}

    ## alertmanager data Persistent Volume existing claim name
    ## Requires alertmanager.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## alertmanager data Persistent Volume mount root path
    ##
    mountPath: /data

    ## alertmanager data Persistent Volume size
    ##
    size: 2Gi

    ## alertmanager data Persistent Volume Storage Class
    ## If defined, storageClassName: &lt;storageClass&gt;
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS &amp; OpenStack)
    ##
    # storageClass: "-"

    ## Subdirectory of alertmanager data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  ## Annotations to be added to alertmanager pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## alertmanager resource requests and limits
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the alertmanager service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    # nodePort: 30000
    type: ClusterIP

## Monitors ConfigMap changes and POSTs to a URL
## Ref: https://github.com/jimmidyson/configmap-reload
##
configmapReload:
  ## configmap-reload container name
  ##
  name: configmap-reload

  ## configmap-reload container image
  ##
  image:
    repository: jimmidyson/configmap-reload
    tag: v0.1
    pullPolicy: IfNotPresent

  ## configmap-reload resource requests and limits
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}

kubeStateMetrics:
  ## If false, kube-state-metrics will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## kube-state-metrics container name
  ##
  name: kube-state-metrics

  ## kube-state-metrics container image
  ##
  image:
    repository: gcr.io/google_containers/kube-state-metrics
    tag: v1.1.0-rc.0
    pullPolicy: IfNotPresent

  ## Node labels for kube-state-metrics pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to kube-state-metrics pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## kube-state-metrics resource requests and limits
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 16Mi
    # requests:
    #   cpu: 10m
    #   memory: 16Mi

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    clusterIP: None

    ## List of IP addresses at which the kube-state-metrics service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    type: ClusterIP

nodeExporter:
  ## If false, node-exporter will not be installed
  ##
  enabled: true

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## node-exporter container name
  ##
  name: node-exporter

  ## node-exporter container image
  ##
  image:
    repository: prom/node-exporter
    tag: v0.15.0
    pullPolicy: IfNotPresent

  ## Additional node-exporter container arguments
  ##
  extraArgs: {}

  ## Additional node-exporter hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: textfile-dir
    #   mountPath: /srv/txt_collector
    #   hostPath: /var/lib/node-exporter
    #   readOnly: true

  ## Node tolerations for node-exporter scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for node-exporter pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to node-exporter pods
  ##
  podAnnotations: {}

  ## node-exporter resource limits &amp; requests
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 200m
    #   memory: 50Mi
    # requests:
    #   cpu: 100m
    #   memory: 30Mi

  service:
    annotations:
      prometheus.io/scrape: "true"
    labels: {}

    clusterIP: None

    ## List of IP addresses at which the node-exporter service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    hostPort: 9100
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9100
    type: ClusterIP

server:
  ## Prometheus server container name
  ##
  name: server

  # Defines the serviceAccountName to use when `rbac.create=false`
  serviceAccountName: default

  ## Prometheus server container image
  ##
  image:
    repository: prom/prometheus
    tag: v1.8.0
    pullPolicy: IfNotPresent

  ## (optional) alertmanager URL
  ## only used if alertmanager.enabled = false
  alertmanagerURL: ""

  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
  ## so that the various internal URLs are still able to access as they are in the default case.
  ## (Optional)
  baseURL: ""

  ## Additional Prometheus server container arguments
  ##
  extraArgs: {}

  ## Additional Prometheus server hostPath mounts
  ##
  extraHostPathMounts: []
    # - name: certs-dir
    #   mountPath: /etc/kubernetes/certs
    #   hostPath: /etc/kubernetes/certs
    #   readOnly: true

  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}
  ## Defining configMapOverrideName will cause templates/server-configmap.yaml
  ## to NOT generate a ConfigMap resource
  ##
  configMapOverrideName: ""

  ingress:
    ## If true, Prometheus server Ingress will be created
    ##
    enabled: false

    ## Prometheus server Ingress annotations
    ##
    annotations: {}
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## Prometheus server Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - prometheus.domain.com

    ## Prometheus server Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-server-tls
    #     hosts:
    #       - prometheus.domain.com

  ## Server Deployment Strategy type
  # strategy:
  #   type: Recreate

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##
  tolerations: []
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  ## Node labels for Prometheus server pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  persistentVolume:
    ## If true, Prometheus server will create/use a Persistent Volume Claim
    ## If false, use emptyDir
    ##
    enabled: true

    ## Prometheus server data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteOnce

    ## Prometheus server data Persistent Volume annotations
    ##
    annotations: {}

    ## Prometheus server data Persistent Volume existing claim name
    ## Requires server.persistentVolume.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim: ""

    ## Prometheus server data Persistent Volume mount root path
    ##
    mountPath: /data

    ## Prometheus server data Persistent Volume size
    ##
    size: 8Gi

    ## Prometheus server data Persistent Volume Storage Class
    ## If defined, storageClassName: &lt;storageClass&gt;
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS &amp; OpenStack)
    ##
    # storageClass: "-"

    ## Subdirectory of Prometheus server data Persistent Volume to mount
    ## Useful if the volume's root directory is not empty
    ##
    subPath: ""

  ## Annotations to be added to Prometheus server pods
  ##
  podAnnotations: {}
    # iam.amazonaws.com/role: prometheus

  replicaCount: 1

  ## Prometheus server resource requests and limits
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 500m
    #   memory: 512Mi
    # requests:
    #   cpu: 500m
    #   memory: 512Mi

  service:
    annotations: {}
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the Prometheus server service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 80
    type: ClusterIP

  ## Prometheus server pod termination grace period
  ##
  terminationGracePeriodSeconds: 300

  ## Prometheus data retention period (i.e 360h)
  ##
  retention: ""

pushgateway:
  ## If false, pushgateway will not be installed
  ##
  enabled: true

  ## pushgateway container name
  ##
  name: pushgateway

  ## pushgateway container image
  ##
  image:
    repository: prom/pushgateway
    tag: v0.4.0
    pullPolicy: IfNotPresent

  ## Additional pushgateway container arguments
  ##
  extraArgs: {}

  ingress:
    ## If true, pushgateway Ingress will be created
    ##
    enabled: false

    ## pushgateway Ingress annotations
    ##
    annotations:
    #   kubernetes.io/ingress.class: nginx
    #   kubernetes.io/tls-acme: 'true'

    ## pushgateway Ingress hostnames
    ## Must be provided if Ingress is enabled
    ##
    hosts: []
    #   - pushgateway.domain.com

    ## pushgateway Ingress TLS configuration
    ## Secrets must be manually created in the namespace
    ##
    tls: []
    #   - secretName: prometheus-alerts-tls
    #     hosts:
    #       - pushgateway.domain.com

  ## Node labels for pushgateway pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## Annotations to be added to pushgateway pods
  ##
  podAnnotations: {}

  replicaCount: 1

  ## pushgateway resource requests and limits
  ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 10m
    #   memory: 32Mi
    # requests:
    #   cpu: 10m
    #   memory: 32Mi

  service:
    annotations:
      prometheus.io/probe: pushgateway
    labels: {}
    clusterIP: ""

    ## List of IP addresses at which the pushgateway service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []

    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    servicePort: 9091
    type: ClusterIP

## alertmanager ConfigMap entries
##
alertmanagerFiles:
  alertmanager.yml: |-
    global:
      # slack_api_url: ''
    receivers:
      - name: default-receiver
        # slack_configs:
        #  - channel: '@you'
        #    send_resolved: true
    route:
      group_wait: 10s
      group_interval: 5m
      receiver: default-receiver
      repeat_interval: 3h
## Prometheus server ConfigMap entries
##
serverFiles:
  alerts: ""
  rules: ""

  prometheus.yml: |-
    rule_files:
      - /etc/config/rules
      - /etc/config/alerts
    scrape_configs:
      - job_name: 'demo-app'
        scrape_interval: 5s
        metrics_path: '/prometheus'
        static_configs:
          - targets:
            - github-analytics.cloudpipelines-prod.svc.cluster.local:8080
      - job_name: prometheus
        static_configs:
          - targets:
            - localhost:9090
      # A scrape configuration for running Prometheus on a Kubernetes cluster.
      # This uses separate scrape configs for cluster components (i.e. API server, node)
      # and services to allow each to use different authentication configs.
      #
      # Kubernetes labels will be added as Prometheus labels on metrics via the
      # `labelmap` relabeling action.
      # Scrape config for API servers.
      #
      # Kubernetes exposes API servers as endpoints to the default/kubernetes
      # service so this uses `endpoints` role and uses relabelling to only keep
      # the endpoints associated with the default/kubernetes service using the
      # default named port `https`. This works for single API server deployments as
      # well as HA API server deployments.
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https
        # This TLS &amp; bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery &amp; scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # &lt;kubernetes_sd_config&gt;.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https
      - job_name: 'kubernetes-nodes'
        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https
        # This TLS &amp; bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery &amp; scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # &lt;kubernetes_sd_config&gt;.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics
      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` &amp; most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: (.+)(?::\d+);(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            action: replace
            target_label: kubernetes_name
      - job_name: 'prometheus-pushgateway'
        honor_labels: true
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: pushgateway
      # Example scrape config for probing services via the Blackbox Exporter.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/probe`: Only probe services that have a value of `true`
      - job_name: 'kubernetes-services'
        metrics_path: /probe
        params:
          module: [http_2xx]
        kubernetes_sd_configs:
          - role: service
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
            action: keep
            regex: true
          - source_labels: [__address__]
            target_label: __param_target
          - target_label: __address__
            replacement: blackbox
          - source_labels: [__param_target]
            target_label: instance
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_service_name]
            target_label: kubernetes_name
      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: (.+):(?:\d+);(\d+)
            replacement: ${1}:${2}
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name
networkPolicy:
  ## Enable creation of NetworkPolicy resources.
  ##
  enabled: false</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Next, create the prometheus installation with the predefined values. To do so, run the following command:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ helm install --name cloudpipelines-prometheus stable/prometheus -f values.yaml</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Then you should see the following output:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">NOTES:
The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:
cloudpipelines-prometheus-prometheus-server.default.svc.cluster.local


Get the Prometheus server URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=server" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9090


The Prometheus alertmanager can be accessed via port 80 on the following DNS name from within your cluster:
cloudpipelines-prometheus-prometheus-alertmanager.default.svc.cluster.local


Get the Alertmanager URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=alertmanager" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9093


The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:
cloudpipelines-prometheus-prometheus-pushgateway.default.svc.cluster.local


Get the PushGateway URL by running these commands in the same shell:
  export POD_NAME=$(kubectl get pods --namespace default -l "app=prometheus,component=pushgateway" -o jsonpath="{.items[0].metadata.name}")
  kubectl --namespace default port-forward $POD_NAME 9093

For more information on running Prometheus, visit:
https://prometheus.io/</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="running-grafana-on-kubernetes"><a class="link" href="#running-grafana-on-kubernetes">Running Grafana on Kubernetes</a></h3>
<div class="paragraph">
<p>Use Helm to install Grafana, by running the following command:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ helm install --name cloudpipelines-grafana stable/grafana</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>You should see the following output:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">NOTES:
1. Get your 'admin' user password by running:

   kubectl get secret --namespace default cloudpipelines-grafana-grafana -o jsonpath="{.data.grafana-admin-password}" | base64 --decode ; echo

2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:

   cloudpipelines-grafana-grafana.default.svc.cluster.local

   Get the Grafana URL to visit by running these commands in the same shell:

     export POD_NAME=$(kubectl get pods --namespace default -l "app=cloudpipelines-grafana-grafana,component=grafana" -o jsonpath="{.items[0].metadata.name}")
     kubectl --namespace default port-forward $POD_NAME 3000

3. Login with the password from step 1 and the username: admin</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Perform the steps listed in the preceding output and add the Grafana&#8217;s datasource
as Prometheus with the following URL: <code><a href="https://cloudpipelines-prometheus-prometheus-server.default.svc.cluster.local" class="bare">https://cloudpipelines-prometheus-prometheus-server.default.svc.cluster.local</a></code></p>
</div>
<div class="paragraph">
<p>You can pick up the dashboard with the Grafana ID (2471). This is the
default dashboard for the Cloud Pipelines demo apps.</p>
</div>
<div class="paragraph">
<p>If you have both apps (<code>github-webhook</code> and <code>github-analytics</code>) running on production,
you can now trigger the messages. Download the JSON with a sample request
from <a href="https://github.com/marcingrzejszczak/github-webhook-kubernetes/blob/master/src/test/resources/github-webhook-input/hook-created.json">the github-webhook repository</a>.
Next, pick one of the <code>github-webhook</code> pods and forward its port
locally to a port <code>9876</code>, as follows:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ kubectl port-forward --namespace=cloudpipelines-prod $( kubectl get pods --namespace=cloudpipelines-prod | grep github-webhook | head -1 | awk '{print $1}' ) 9876:8080</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Next, send a couple of requests (more than four), by using cURL as follows:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ curl -X POST http://localhost:9876/ -d @path/to/issue-created.json \
--header "Content-Type: application/json"</code></pre>
</div>
</div>
</div>
</div>
<div class="paragraph">
<p>Then, if you use Grafana, you can see that you went above the threshold.</p>
</div>
</div>
</div>
</div>
</div>
<link rel="stylesheet" href="js/highlight/styles/atom-one-dark-reasonable.min.css">
<script src="js/highlight/highlight.min.js"></script>
<script>hljs.initHighlighting()</script>
</body>
</html>